<h2>A benchmark for single-machine test-case generation tools</h2>

<p>This project contains a benchmark aimed at comparing multiple combinatorial
test generation tools on the same set of problem instances. In a nutshell,
combinatorial test case generation consists of:</p>

<ul>
<li>a number <i>n</i> of <strong>parameters</strong> noted <i>p</i><sub>1</sub>, &hellip;
<i>p</i><sub><i>n</i></sub></li>
<li>each parameter can take a number of predefined <strong>values</strong>;
 the problem is called
 <em>uniform</em> if all parameters have the same number <i>v</i> of possible values</li>
<li>an <strong>interaction strength</strong> <i>t</i></li>
</ul>

<p>A test case sets a value to each parameter; a test suite is a set of test cases.
The goal of combinatorial test generation is to produce the smallest test suite
such that, for any set of <i>t</i> parameters, all value combinations are present in
at least one test case. For more information, the reader is directed to NIST's
website, which has a good
<a href="https://csrc.nist.gov/Projects/automated-combinatorial-testing-for-software/interactions-involved-in-software-failures">introduction</a>
on the subject.</p>

<p>Multiple tools have been developed to generate such test suites, and the aim of
this benchmark is to compare two new algorithms, based on graph reductions, to
the set of existing and freely-available generators. For each test generation
problem, the two important factors that are measured are:</p>

<ol>
<li>the size of the test suite produced by each tool (the smaller the better)</li>
<li>the time taken to obtain this test suite (again, the smaller the better)</li>
</ol>

<h3>Publications</h3>

<p>The data generated by this benchmark are part of the experimental results
presented in the following publication:</p>

<blockquote>
E. La Chance, S. Hallé. (2020). Extended Combinatorial Test Case Generation
by Graph Reductions. Submitted to <i>Software Testing, Verification and
Reliability</i>.
</blockquote>

<p>An earlier version of these experiments has been the subject of another
publication:</p>

<blockquote>
S. Hallé, E. La Chance, S. Gaboury. (2015). Graph Methods for Generating Test
Cases with Universal and Existential Constraints. Proc. ICTSS 2015: 55-70.
DOI: 10.1007/978-3-319-25945-1_4
</blockquote>

<h3>Tools included in the benchmark</h3>

<p>The benchmark is designed to compare the following tools:</p>

<ul>
<li>DSATUR, a home implementation of a graph coloring algorithm in C++</li>
<li><a href="https://github.com/bdesham/hitting-set">hitting-set</a>,
a hypergraph vertex cover algorithm</li>
<li>A <a href="https://github.com/sylvainhalle/QICT">forked version</a> of
  <a href="http://msdn.microsoft.com/en-us/magazine/ee819137.aspx">QICT</a></li>
<li>A stand-alone version of
  <a href="https://github.com/sylvainhalle/Tcases-Standalone">Tcases</a> 1.3.0</li>
<li><a href="http://burtleburtle.net/bob/math/jenny.html">Jenny</a>, compiled with GCC
  from February 5, 2005 version</li>
<li><a href="http://www.mcdowella.demon.co.uk/allPairs.html">AllPairs</a>
 (website no longer exists, use the <a href="https://archive.org/">Wayback Machine</a>)
  A runnable JAR was created out of the hefty archive that contains the program. The
  program is not dated and has no version number; we retrieved it on
  January 17th, 2015.</li>
<li><a href="https://csrc.nist.gov/Projects/automated-combinatorial-testing-for-software/downloadable-tools">ACTS</a>
  version 3.1. The tool is freely available, but the authors must be asked first.</li>
 </ul>
 
 <h3>Structure of this benchmark</h3>
 
 <p>This program is an instance of a laboratory in the LabPal environment.
 You must first have some knowledge of how LabPal works; please refer to the
 <a href="/help">Help</a> page and to the online documentation of LabPal itself.</p>
 
 <p>In this lab, a combinatorial testing problem consists of a specific set of
 values for <i>n</i>, <i>t</i> and <i>v</i>. Some of the problems contain
 additional <em>constraints</em>:</p>
 
 <ul>
 <li>Forbidden tuples: generate a test suite where some combinations of values
 for some parameters must not be present</li>
 <li>Increasing values: make sure that a generic condition on parameter values
 applies on each test case</li>
 <li>Test suite completion: generate the best test suite that includes a set
 of pre-existing test cases</li>
 </ul>
 
 <p>Some of these testing problems have additional parameters that define them.</p>
 
 <p>An <a href="/experiments">experiment</a> consists of a single tool generating
 a test suite for a single testing problem. Results from these experiments are
 fetched and grouped in various ways, in order to compare the tools across multiple
 dimensions (e.g.: varying <i>n</i>, varying <i>t</i>, etc.). These results are
 presented in <a href="/tables">tables</a> and <a href="/plots">plots</a>.</p>
 
 <p>Most of the aforementioned tools require that a test generation problem be
 provided through an input file; for the same problem, the format accepted by each tool
 differs widely. When an experiment for a particular tool is run for the first time,
 the lab will first generate the input file for the corresponding problem, and
 save it to a local folder called <tt>data</tt>. The naming convention for these
 input files is as follows:</p>
 
 <blockquote>
 <tt>ttttt-ppppp-t-n-v-xxxx.ext</tt>
 </blockquote>
 
 <p>where:</p>
 
 <ul>
 <li><tt>ttttt</tt>: the name of the testing tool</li>
 <li><tt>ppppp</tt>: the name of the generic testing problem</li>
 <li><tt>t</tt>, <tt>n</tt>, <tt>v</tt>: the basic parameters of the combinatorial testing problem</li>
 <li><tt>xxxx</tt>: values of any additional parameters defining the problem instance</li>
 <li><tt>ext</tt>: a file extension specific to the tool (e.g. <tt>xml</tt>, <tt>txt</tt>, etc.)</li>
 </ul>
 
 <p>If an experiment is run for the second time, and that the corresponding file
 already exists in the <tt>data</tt> folder, it is not re-generated. As a rule, it
 is safe to delete this folder when the lab is not in use; any missing files will be
 re-generated the next time the lab is run. The time taken to
 generate the input file is not counted in the running time for each tool.</p>